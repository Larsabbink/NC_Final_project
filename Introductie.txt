Introductie

First of all lets look at what checkers is, I think most of you probably have already played it.
There are a lot of different variants of checkers, but for this project i will be using the english/american version. This version is played on a 8x8 board where each player starts with 12 pieces. The pieces are only allowed to move on the black squares so they always move diagonally.
A player can only move 1 square except when they take another piece. If a player is able to take another piece he/she must take that piece. When a piece reaches the final row it becomes a king and from then on is able to also move backwards.
A player wins the game when the opponent has no pieces left or is not able to move anymore. The game ends in a draw after 40 moves without a capture. This variant of checkers is already weakly solved meaning that given the standards starting position and perfect play each player can guarantee a draw. However, since there are so many variants of checkers which are not yet solved it is still interesting to do some research about checkers.

For the model I use a simple neural network consisting of 3 linear layers, where after the first two layers there is a ReLU activation layer and after the last linear layer a softmax layer to get predictions between 0 and 1.
THe input of the model consists of a flattend 5*8*3 matrix. We get the 8*4 matrix because the board is 8*8 but you only play on the black squares so we can halve this. The first 8*4 layer contains the location of pieces of the current player, the second layer contains the opponents pieces. The third layer contains the location of the current players king pieces and the fourth layer contains the opponents king pieces. The last 8*4 layer contains the binary representation of the amount of moves made since the last capture, I currently don't know how usefull this is so this might get removed in a later version.

I'm currently still working at the evolutionary algorithm so I'm not quite sure how it is going to look like, but most of the experimenting will be with these parameters. The algorithm will start with a population of K networks, each network will play n games against randomly selected opponents. If a network wins the game he gets 1 point with a loss 0 points and with a draw the players will both get a score. In case of a draw the player with more pieces than the opponent will get a slightly higher score than the opponent.
After this small tournament the best networks are selected and their parameters are mutated with a probability p to create a new population.
To evaluate if the network actually is able to learn I will most likely let the best network play against an opponent who randomly picks one of the available legal moves. If the win-rate goes up against such an opponent we can conclude that the model is actually able to learn.